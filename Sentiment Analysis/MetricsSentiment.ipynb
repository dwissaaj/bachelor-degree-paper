{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset indonlu (C:\\Users\\W I N D O W S\\.cache\\huggingface\\datasets\\indonlu\\smsa\\1.0.0\\0a83b181cd831cd5d9c15ffe39f3be76af23407eba2c902bccca53fa905d68af)\n",
      "Loading cached shuffled indices for dataset at C:\\Users\\W I N D O W S\\.cache\\huggingface\\datasets\\indonlu\\smsa\\1.0.0\\0a83b181cd831cd5d9c15ffe39f3be76af23407eba2c902bccca53fa905d68af\\cache-7bb4ac8c0cddcd85.arrow\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from evaluate import evaluator\n",
    "from transformers import AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "data = load_dataset(\"indonlu\",\"smsa\", split=\"test\").shuffle(seed=42).select(range(500))\n",
    "task_evaluator = evaluator(\"text-classification\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=\"mdhugol/indonesia-bert-sentiment-classification\",\n",
    "    data=data,\n",
    "    label_mapping={'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "ClassLabel(num_classes=3, names=['positive', 'neutral', 'negative'], id=None)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.features[\"label\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "failed\n",
    "{'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "{'positive': 0.0, 'neutral': 1.0, 'negative': 2.0}\n",
    "\"\"\"\n",
    "coba tes\n",
    "mirip model\n",
    "{'LABEL_0': 'positive', 'LABEL_1': 'neutral', 'LABEL_2': 'negative'}\n",
    "\n",
    "\n",
    "berhasil\n",
    "label_mapping={'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "pipe = pipeline(\"text-classification\", model=\"mdhugol/indonesia-bert-sentiment-classification\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"mdhugol/indonesia-bert-sentiment-classification\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn [13], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mevaluate\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m eval_results \u001B[38;5;241m=\u001B[39m \u001B[43mtask_evaluator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_or_pipeline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpipe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcombine\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maccuracy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mf1\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlabel_mapping\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mLABEL_0\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mLABEL_1\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mLABEL_2\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m}\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\w i n d o w s\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages\\evaluate\\evaluator\\text_classification.py:105\u001B[0m, in \u001B[0;36mTextClassificationEvaluator.compute\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m     41\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mfloat\u001B[39m], Any]:\n\u001B[0;32m     42\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;124;03m    Compute the metric for a given pipeline and dataset combination.\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;124;03m    >>> )\u001B[39;00m\n\u001B[0;32m    103\u001B[0m \u001B[38;5;124;03m    ```\"\"\"\u001B[39;00m\n\u001B[1;32m--> 105\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32mc:\\users\\w i n d o w s\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages\\evaluate\\evaluator\\base.py:205\u001B[0m, in \u001B[0;36mEvaluator.compute\u001B[1;34m(self, model_or_pipeline, data, metric, tokenizer, feature_extractor, strategy, confidence_level, n_resamples, device, random_state, input_column, label_column, label_mapping)\u001B[0m\n\u001B[0;32m    202\u001B[0m metric_inputs\u001B[38;5;241m.\u001B[39mupdate(predictions)\n\u001B[0;32m    204\u001B[0m \u001B[38;5;66;03m# Compute metrics from references and predictions\u001B[39;00m\n\u001B[1;32m--> 205\u001B[0m metric_results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_metric\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    206\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetric\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    207\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetric_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    208\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstrategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    209\u001B[0m \u001B[43m    \u001B[49m\u001B[43mconfidence_level\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfidence_level\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    210\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_resamples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_resamples\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    211\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrandom_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    212\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    214\u001B[0m result\u001B[38;5;241m.\u001B[39mupdate(metric_results)\n\u001B[0;32m    215\u001B[0m result\u001B[38;5;241m.\u001B[39mupdate(perf_results)\n",
      "File \u001B[1;32mc:\\users\\w i n d o w s\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages\\evaluate\\evaluator\\base.py:343\u001B[0m, in \u001B[0;36mEvaluator.compute_metric\u001B[1;34m(self, metric, metric_inputs, strategy, confidence_level, n_resamples, random_state)\u001B[0m\n\u001B[0;32m    333\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_metric\u001B[39m(\n\u001B[0;32m    334\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    335\u001B[0m     metric: EvaluationModule,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    340\u001B[0m     random_state: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    341\u001B[0m ):\n\u001B[0;32m    342\u001B[0m     \u001B[38;5;124;03m\"\"\"Compute and return metrics.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 343\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43mmetric\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmetric_inputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mMETRIC_KWARGS\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    345\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m strategy \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbootstrap\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    346\u001B[0m         metric_keys \u001B[38;5;241m=\u001B[39m result\u001B[38;5;241m.\u001B[39mkeys()\n",
      "File \u001B[1;32mc:\\users\\w i n d o w s\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages\\evaluate\\module.py:859\u001B[0m, in \u001B[0;36mCombinedEvaluations.compute\u001B[1;34m(self, predictions, references, **kwargs)\u001B[0m\n\u001B[0;32m    857\u001B[0m     batch \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpredictions\u001B[39m\u001B[38;5;124m\"\u001B[39m: predictions, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreferences\u001B[39m\u001B[38;5;124m\"\u001B[39m: references, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs}\n\u001B[0;32m    858\u001B[0m     batch \u001B[38;5;241m=\u001B[39m {input_name: batch[input_name] \u001B[38;5;28;01mfor\u001B[39;00m input_name \u001B[38;5;129;01min\u001B[39;00m evaluation_module\u001B[38;5;241m.\u001B[39m_feature_names()}\n\u001B[1;32m--> 859\u001B[0m     results\u001B[38;5;241m.\u001B[39mappend(\u001B[43mevaluation_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    861\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_merge_results(results)\n",
      "File \u001B[1;32mc:\\users\\w i n d o w s\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages\\evaluate\\module.py:444\u001B[0m, in \u001B[0;36mEvaluationModule.compute\u001B[1;34m(self, predictions, references, **kwargs)\u001B[0m\n\u001B[0;32m    442\u001B[0m inputs \u001B[38;5;241m=\u001B[39m {input_name: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[input_name] \u001B[38;5;28;01mfor\u001B[39;00m input_name \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_feature_names()}\n\u001B[0;32m    443\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m temp_seed(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mseed):\n\u001B[1;32m--> 444\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_compute\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcompute_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    446\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_writer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    447\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuf_writer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--f1\\0ca73f6cf92ef5a268320c697f7b940d1030f8471714bffdb6856c641b818974\\f1.py:127\u001B[0m, in \u001B[0;36mF1._compute\u001B[1;34m(self, predictions, references, labels, pos_label, average, sample_weight)\u001B[0m\n\u001B[0;32m    126\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_compute\u001B[39m(\u001B[38;5;28mself\u001B[39m, predictions, references, labels\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, pos_label\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, average\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m, sample_weight\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m--> 127\u001B[0m     score \u001B[38;5;241m=\u001B[39m \u001B[43mf1_score\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreferences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpredictions\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpos_label\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maverage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\n\u001B[0;32m    129\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mf1\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mfloat\u001B[39m(score) \u001B[38;5;28;01mif\u001B[39;00m score\u001B[38;5;241m.\u001B[39msize \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m score}\n",
      "File \u001B[1;32mc:\\users\\w i n d o w s\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1136\u001B[0m, in \u001B[0;36mf1_score\u001B[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001B[0m\n\u001B[0;32m   1001\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mf1_score\u001B[39m(\n\u001B[0;32m   1002\u001B[0m     y_true,\n\u001B[0;32m   1003\u001B[0m     y_pred,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1009\u001B[0m     zero_division\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarn\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1010\u001B[0m ):\n\u001B[0;32m   1011\u001B[0m     \u001B[38;5;124;03m\"\"\"Compute the F1 score, also known as balanced F-score or F-measure.\u001B[39;00m\n\u001B[0;32m   1012\u001B[0m \n\u001B[0;32m   1013\u001B[0m \u001B[38;5;124;03m    The F1 score can be interpreted as a harmonic mean of the precision and\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1134\u001B[0m \u001B[38;5;124;03m    array([0.66666667, 1.        , 0.66666667])\u001B[39;00m\n\u001B[0;32m   1135\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1136\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfbeta_score\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1137\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1138\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1139\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1140\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1141\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpos_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpos_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1142\u001B[0m \u001B[43m        \u001B[49m\u001B[43maverage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1143\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1144\u001B[0m \u001B[43m        \u001B[49m\u001B[43mzero_division\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mzero_division\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1145\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\w i n d o w s\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1277\u001B[0m, in \u001B[0;36mfbeta_score\u001B[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001B[0m\n\u001B[0;32m   1148\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfbeta_score\u001B[39m(\n\u001B[0;32m   1149\u001B[0m     y_true,\n\u001B[0;32m   1150\u001B[0m     y_pred,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1157\u001B[0m     zero_division\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwarn\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1158\u001B[0m ):\n\u001B[0;32m   1159\u001B[0m     \u001B[38;5;124;03m\"\"\"Compute the F-beta score.\u001B[39;00m\n\u001B[0;32m   1160\u001B[0m \n\u001B[0;32m   1161\u001B[0m \u001B[38;5;124;03m    The F-beta score is the weighted harmonic mean of precision and recall,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1274\u001B[0m \u001B[38;5;124;03m    array([0.71..., 0.        , 0.        ])\u001B[39;00m\n\u001B[0;32m   1275\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1277\u001B[0m     _, _, f, _ \u001B[38;5;241m=\u001B[39m \u001B[43mprecision_recall_fscore_support\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1278\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1279\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1280\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbeta\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1281\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1282\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpos_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpos_label\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1283\u001B[0m \u001B[43m        \u001B[49m\u001B[43maverage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1284\u001B[0m \u001B[43m        \u001B[49m\u001B[43mwarn_for\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mf-score\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1285\u001B[0m \u001B[43m        \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1286\u001B[0m \u001B[43m        \u001B[49m\u001B[43mzero_division\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mzero_division\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1287\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1288\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f\n",
      "File \u001B[1;32mc:\\users\\w i n d o w s\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1563\u001B[0m, in \u001B[0;36mprecision_recall_fscore_support\u001B[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001B[0m\n\u001B[0;32m   1561\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m beta \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m   1562\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbeta should be >=0 in the F-beta score\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1563\u001B[0m labels \u001B[38;5;241m=\u001B[39m \u001B[43m_check_set_wise_labels\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_pred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpos_label\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1565\u001B[0m \u001B[38;5;66;03m# Calculate tp_sum, pred_sum, true_sum ###\u001B[39;00m\n\u001B[0;32m   1566\u001B[0m samplewise \u001B[38;5;241m=\u001B[39m average \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msamples\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[1;32mc:\\users\\w i n d o w s\\pycharmprojects\\pythonproject\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1381\u001B[0m, in \u001B[0;36m_check_set_wise_labels\u001B[1;34m(y_true, y_pred, average, labels, pos_label)\u001B[0m\n\u001B[0;32m   1379\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m y_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulticlass\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m   1380\u001B[0m             average_options\u001B[38;5;241m.\u001B[39mremove(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msamples\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1381\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1382\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTarget is \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m but average=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. Please \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1383\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mchoose another average setting, one of \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (y_type, average_options)\n\u001B[0;32m   1384\u001B[0m         )\n\u001B[0;32m   1385\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m pos_label \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m   1386\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m   1387\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNote that pos_label (set to \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m) is ignored when \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1388\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maverage != \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m (got \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m). You may use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1391\u001B[0m         \u001B[38;5;167;01mUserWarning\u001B[39;00m,\n\u001B[0;32m   1392\u001B[0m     )\n",
      "\u001B[1;31mValueError\u001B[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "eval_results = task_evaluator.compute(\n",
    "    model_or_pipeline=pipe,\n",
    "    data=data,\n",
    "    label_mapping={'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2}\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(eval_results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
